{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import spacy\nimport pdfplumber\n\nmodel = spacy.load('fr_core_news_md')\n\nfinwords = [\"Chiffre d’affaires net\", \"Free cash\", \"Résultat opérationnel\", \"Résultat net core\", \"BPA core\"]\n\nwith pdfplumber.open(\"/kaggle/input/pdfsds/PDFs/q4-2021-media-release-fr.pdf\") as pdf:\n    text = \"\"\n    for page in pdf.pages:\n        text += page.extract_text()\n\ndoc = model(text)\n\nfor ent in doc.ents:\n    if ent.label_ == \"MISC\" and any(finword.lower() in ent.text.lower() for finword in finwords):\n        print(f\"Entity: {ent.text}, Label: {ent.label_}\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python -m spacy download fr_core_news_md","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import spacy\n\n# Load the French language model for sentence segmentation\nnlp = spacy.load(\"fr_core_news_md\")\n\n# Your financial terms mapping (for BIO tagging)\nfinancial_terms = {\n    \"dette nette\": \"NetDebt\",\n    \"résultat opérationnel\": \"OperatingProfit\",\n    \"actifs financiers\": \"FinancialAssets\",\n    \"passifs courants\": \"CurrentLiabilities\"\n}\n\n# Function to tag entities in the text using BIO tagging\ndef preprocess_bio_tagging(text):\n    # Step 1: Split the text into sentences using spaCy\n    doc = nlp(text)\n    sentences = [sent.text.strip() for sent in doc.sents]  # List of sentences\n\n    # Step 2: Process each sentence for BIO tagging\n    tagged_sentences = []\n    \n    for sentence in sentences:\n        # Split the sentence into tokens (words)\n        tokens = sentence.split()\n        tags = [\"O\"] * len(tokens)  # Default tag is \"O\" (Outside)\n        \n        # Step 3: Identify financial terms in the tokens and apply BIO tagging\n        for term, tag in financial_terms.items():\n            term_tokens = term.split()\n            if ' '.join(tokens).find(term) != -1:  # Check if term is in the sentence\n                # Find the starting index of the term in the sentence\n                start_idx = tokens.index(term_tokens[0])\n                \n                # Assign BIO tags\n                tags[start_idx] = f\"B-{tag}\"\n                for i in range(1, len(term_tokens)):\n                    tags[start_idx + i] = f\"I-{tag}\"\n        \n        # Append the tokens and their corresponding BIO tags to the result\n        tagged_sentence = list(zip(tokens, tags))\n        tagged_sentences.append(tagged_sentence)\n\n    return tagged_sentences\n\n\n\npdf_text = \"La dette nette a augmenté de 5 millions d'euros. Le résultat opérationnel a grimpé à USD 2,6 milliards (+45%, +51% tcc), grâce principalement à une diminution des pertes de valeur et des charges pour litiges ainsi qu’au produit de créances éventuelles. \"\n\ntagged_data = preprocess_bio_tagging(pdf_text)\n\n# Print the tagged data\nfor sentence in tagged_data:\n    print(sentence)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install transformers datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T09:23:14.696514Z","iopub.execute_input":"2024-12-30T09:23:14.696898Z","iopub.status.idle":"2024-12-30T09:23:18.843039Z","shell.execute_reply.started":"2024-12-30T09:23:14.696866Z","shell.execute_reply":"2024-12-30T09:23:18.841420Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (18.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import json\n\n# Example: Input tokens, labels, and attention mask\ninput_ids = [[5, 54, 1206, 9870, 33, 6834, 8, 205, 713, 18, 11, 1958, 9, 6, 1, 1, 1, 1, 1, 1]]\nlabels = [[0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\nattention_masks = [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]]\n\n# Corresponding tokens (can be obtained from tokenizer.convert_ids_to_tokens)\ntokens = [\n    [\"<s>\", \"Le\", \"résultat\", \"opérationnel\", \"a\", \"augmenté\", \"de\", \"5\", \"millions\", \"d'\", \"euros\", \".\", \"</s>\"]\n]\n\n# Map integer labels back to tags (reverse mapping)\nlabel_map_reverse = {0: \"O\", 1: \"B-OperatingProfit\", 2: \"I-OperatingProfit\"}\n\n# Prepare data in a structured format\ndata = []\nfor i in range(len(tokens)):\n    sentence_data = {\n        \"tokens\": tokens[i],\n        \"input_ids\": input_ids[i],\n        \"attention_mask\": attention_masks[i],\n        \"tags\": [label_map_reverse[label] for label in labels[i] if label != -100]  # Ignore -100 special tokens\n    }\n    data.append(sentence_data)\n\n# Save to a JSON file\nwith open(\"bio_tagged_data.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(data, f, indent=4, ensure_ascii=False)\n\nprint(\"BIO-tagged data saved to bio_tagged_data.json\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T09:24:10.000282Z","iopub.execute_input":"2024-12-30T09:24:10.000635Z","iopub.status.idle":"2024-12-30T09:24:10.010614Z","shell.execute_reply.started":"2024-12-30T09:24:10.000605Z","shell.execute_reply":"2024-12-30T09:24:10.009500Z"}},"outputs":[{"name":"stdout","text":"BIO-tagged data saved to bio_tagged_data.json\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from datasets import Dataset\n\n# Load the JSON file into a Hugging Face Dataset\nwith open(\"bio_tagged_data.json\", \"r\", encoding=\"utf-8\") as f:\n    bio_data = json.load(f)\n\n# Convert the JSON data into a Hugging Face Dataset\ndataset = Dataset.from_list(bio_data)\n\n# Ensure it contains 'input_ids', 'attention_mask', and 'tags' fields for training\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T09:24:31.980925Z","iopub.execute_input":"2024-12-30T09:24:31.981344Z","iopub.status.idle":"2024-12-30T09:24:33.219880Z","shell.execute_reply.started":"2024-12-30T09:24:31.981312Z","shell.execute_reply":"2024-12-30T09:24:33.218668Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from transformers import CamembertForTokenClassification, CamembertTokenizerFast\n\n# Load the CamemBERT tokenizer and model\ntokenizer = CamembertTokenizerFast.from_pretrained(\"camembert-base\")\nmodel = CamembertForTokenClassification.from_pretrained(\n    \"camembert-base\",\n    num_labels=len(label_map),  # Number of unique tags in your dataset\n    id2label=label_map_reverse,  # Map label IDs to tag names\n    label2id=label_map           # Map tag names to label IDs\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T09:24:36.722774Z","iopub.execute_input":"2024-12-30T09:24:36.723191Z","iopub.status.idle":"2024-12-30T09:24:37.381663Z","shell.execute_reply.started":"2024-12-30T09:24:36.723156Z","shell.execute_reply":"2024-12-30T09:24:37.380170Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-ede458804c8b>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load the CamemBERT tokenizer and model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCamembertTokenizerFast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"camembert-base\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m model = CamembertForTokenClassification.from_pretrained(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;34m\"camembert-base\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Number of unique tags in your dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3352\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3353\u001b[0m             \u001b[0mconfig_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3354\u001b[0;31m             config, model_kwargs = cls.config_class.from_pretrained(\n\u001b[0m\u001b[1;32m   3355\u001b[0m                 \u001b[0mconfig_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3356\u001b[0m                 \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[1;32m    608\u001b[0m             )\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mfrom_dict\u001b[0;34m(cls, config_dict, **kwargs)\u001b[0m\n\u001b[1;32m    780\u001b[0m             \u001b[0mid2label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id2label\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id2label\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid2label\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 782\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    783\u001b[0m                     \u001b[0;34mf\"You passed along `num_labels={num_labels }` with an incompatible id to label map: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m                     \u001b[0;34mf\"{kwargs['id2label']}. Since those arguments are inconsistent with each other, you should remove \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: You passed along `num_labels=5` with an incompatible id to label map: {0: 'O', 1: 'B-OperatingProfit', 2: 'I-OperatingProfit'}. Since those arguments are inconsistent with each other, you should remove one of them."],"ename":"ValueError","evalue":"You passed along `num_labels=5` with an incompatible id to label map: {0: 'O', 1: 'B-OperatingProfit', 2: 'I-OperatingProfit'}. Since those arguments are inconsistent with each other, you should remove one of them.","output_type":"error"}],"execution_count":11},{"cell_type":"code","source":"# Define the labels and their corresponding IDs\nlabels = [\"O\", \"B-OperatingProfit\", \"I-OperatingProfit\"]  # Add all unique tags in your dataset\nlabel_map = {label: idx for idx, label in enumerate(labels)}  # Label to ID\nlabel_map_reverse = {idx: label for label, idx in label_map.items()}  # ID to Label\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T09:25:17.963677Z","iopub.execute_input":"2024-12-30T09:25:17.964080Z","iopub.status.idle":"2024-12-30T09:25:17.969787Z","shell.execute_reply.started":"2024-12-30T09:25:17.964044Z","shell.execute_reply":"2024-12-30T09:25:17.968496Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"from transformers import CamembertForTokenClassification, CamembertTokenizerFast\n\n# Load tokenizer\ntokenizer = CamembertTokenizerFast.from_pretrained(\"camembert-base\")\n\n# Load model with consistent labels\nmodel = CamembertForTokenClassification.from_pretrained(\n    \"camembert-base\",\n    num_labels=len(labels),  # Ensure num_labels matches the length of `labels`\n    id2label=label_map_reverse,  # Map IDs to labels\n    label2id=label_map  # Map labels to IDs\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T09:25:37.488635Z","iopub.execute_input":"2024-12-30T09:25:37.489073Z","iopub.status.idle":"2024-12-30T09:25:40.231810Z","shell.execute_reply.started":"2024-12-30T09:25:37.488992Z","shell.execute_reply":"2024-12-30T09:25:40.230557Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/445M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6a5b893fe344f74afbca9da8aa50e75"}},"metadata":{}},{"name":"stderr","text":"Some weights of CamembertForTokenClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./camembert-finetuned\",  # Directory to save the model\n    evaluation_strategy=\"steps\",\n    eval_steps=500,\n    logging_dir=\"./logs\",\n    logging_steps=100,\n    save_steps=1000,\n    save_total_limit=2,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    learning_rate=5e-5,\n    warmup_steps=500,\n    gradient_accumulation_steps=2,\n    load_best_model_at_end=True\n)\n\n# Initialize the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"validation\"],\n    tokenizer=tokenizer,\n)\nz\n# Start fine-tuning\ntrainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T09:25:42.753608Z","iopub.execute_input":"2024-12-30T09:25:42.754029Z","iopub.status.idle":"2024-12-30T09:25:54.414654Z","shell.execute_reply.started":"2024-12-30T09:25:42.753972Z","shell.execute_reply":"2024-12-30T09:25:54.413178Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-d3eabb62f525>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenized_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0meval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenized_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"validation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'tokenized_dataset' is not defined"],"ename":"NameError","evalue":"name 'tokenized_dataset' is not defined","output_type":"error"}],"execution_count":16},{"cell_type":"code","source":"trainer.save_model(\"./camembert-finetuned\")\ntokenizer.save_pretrained(\"./camembert-finetuned\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import pipeline\n\n# Load the fine-tuned model\nner_model = CamembertForTokenClassification.from_pretrained(\"./camembert-finetuned\")\nner_tokenizer = CamembertTokenizerFast.from_pretrained(\"./camembert-finetuned\")\n\n# Create a pipeline for NER\nner_pipeline = pipeline(\"ner\", model=ner_model, tokenizer=ner_tokenizer, aggregation_strategy=\"simple\")\n\n# Test on new text\ntest_text = \"Le résultat opérationnel a augmenté de 5 millions d'euros.\"\npredictions = ner_pipeline(test_text)\n\nprint(predictions)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}