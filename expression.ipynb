{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10306101,"sourceType":"datasetVersion","datasetId":6379701}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install transformers datasets torch scikit-learn\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\n# Load dataset\nwith open(\"/kaggle/input/jsonexpre/dataset.json\", \"r\", encoding=\"utf-8\") as f:\n    data = json.load(f)\n    \nprint(data)\nfrom datasets import Dataset\n# Convert the JSON into a Hugging Face Dataset\ndataset = Dataset.from_list(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T18:48:20.426835Z","iopub.execute_input":"2024-12-26T18:48:20.427216Z","iopub.status.idle":"2024-12-26T18:48:20.441023Z","shell.execute_reply.started":"2024-12-26T18:48:20.427188Z","shell.execute_reply":"2024-12-26T18:48:20.439868Z"}},"outputs":[{"name":"stdout","text":"[{'expression': 'R√©serve l√©gale', 'tag': 'StatutoryReserve'}, {'expression': 'R√©serve de capitaux', 'tag': 'CapitalReserve'}, {'expression': 'Capital vers√© additionnel', 'tag': 'AdditionalPaidinCapital'}, {'expression': 'Actifs courants', 'tag': 'CurrentAssets'}]\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\nmodel_name = \"bert-base-multilingual-cased\"  # or \"yiyanghkust/finbert-tone\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T18:50:08.841990Z","iopub.execute_input":"2024-12-26T18:50:08.842531Z","iopub.status.idle":"2024-12-26T18:50:09.247325Z","shell.execute_reply.started":"2024-12-26T18:50:08.842478Z","shell.execute_reply":"2024-12-26T18:50:09.245961Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def tokenize_data(example):\n    return tokenizer(example[\"expression\"], truncation=True, padding=\"max_length\", max_length=128)\n\ntokenized_dataset = dataset.map(tokenize_data, batched=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T18:50:22.421834Z","iopub.execute_input":"2024-12-26T18:50:22.422177Z","iopub.status.idle":"2024-12-26T18:50:22.515798Z","shell.execute_reply.started":"2024-12-26T18:50:22.422152Z","shell.execute_reply":"2024-12-26T18:50:22.514534Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea31711abdcc41b0b537c2e568c14adb"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# Create a mapping from tag to ID\nunique_tags = list(set([entry[\"tag\"] for entry in data]))\ntag_to_id = {tag: idx for idx, tag in enumerate(unique_tags)}\n\n# Add numerical labels to the dataset\ndef add_labels(example):\n    example[\"label\"] = tag_to_id[example[\"tag\"]]\n    return example\n\nlabeled_dataset = tokenized_dataset.map(add_labels)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T18:50:38.316449Z","iopub.execute_input":"2024-12-26T18:50:38.317001Z","iopub.status.idle":"2024-12-26T18:50:38.353467Z","shell.execute_reply.started":"2024-12-26T18:50:38.316947Z","shell.execute_reply":"2024-12-26T18:50:38.352008Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f686ff0a7313443d98b54f9c7bdd4396"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"from datasets import DatasetDict\n\n# Split into train and validation sets\ndataset_split = labeled_dataset.train_test_split(test_size=0.2)\ntrain_dataset = dataset_split[\"train\"]\nval_dataset = dataset_split[\"test\"]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T18:50:55.210397Z","iopub.execute_input":"2024-12-26T18:50:55.210804Z","iopub.status.idle":"2024-12-26T18:50:55.227080Z","shell.execute_reply.started":"2024-12-26T18:50:55.210765Z","shell.execute_reply":"2024-12-26T18:50:55.225899Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name,\n    num_labels=len(unique_tags)  \n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Model architecture:\\n{model}\")\nprint(f\"\\nNumber of parameters: {model.num_parameters():,}\")\nprint(f\"Number of trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T19:28:24.314966Z","iopub.execute_input":"2024-12-26T19:28:24.315390Z","iopub.status.idle":"2024-12-26T19:28:24.324845Z","shell.execute_reply.started":"2024-12-26T19:28:24.315358Z","shell.execute_reply":"2024-12-26T19:28:24.323613Z"}},"outputs":[{"name":"stdout","text":"Model architecture:\nBertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=4, bias=True)\n)\n\nNumber of parameters: 177,856,516\nNumber of trainable parameters: 177,856,516\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=5,\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    load_best_model_at_end=True\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T19:31:02.295451Z","iopub.execute_input":"2024-12-26T19:31:02.295908Z","iopub.status.idle":"2024-12-26T19:31:02.305685Z","shell.execute_reply.started":"2024-12-26T19:31:02.295874Z","shell.execute_reply":"2024-12-26T19:31:02.304438Z"},"_kg_hide-input":true},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer\n)\n\ntrainer.train()\n","metadata":{"trusted":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import Trainer\nfrom tqdm import tqdm\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer\n)\n\ntotal_steps = trainer.args.num_train_epochs * len(train_dataset) // trainer.args.per_device_train_batch_size\n\nwith tqdm(total=total_steps, desc=\"Training Progress\") as pbar:\n    \n    for epoch in range(int(trainer.args.num_train_epochs)):\n        trainer.train()\n        pbar.update(len(train_dataset) // trainer.args.per_device_train_batch_size)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results = trainer.evaluate()\nprint(results)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained(\"./AmelkisTAG\")\ntokenizer.save_pretrained(\"./AmelkisTAG\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import pipeline\n\n# Load the trained model\nclassifier = pipeline(\"text-classification\", model=\"./AmelkisTAG\", tokenizer=\"./AmelkisTAG\", return_all_scores=True)\n\n# Predict\ntext = \"R√©serve l√©gale\"\nprediction = classifier(text)\nprint(prediction)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}